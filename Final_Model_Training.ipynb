{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og5vZT-fVszA",
        "outputId": "2ea03cae-2fbb-44fb-9f4b-dc9370863015"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for NN model trained with oversampling of cleaned dataset with additional features"
      ],
      "metadata": {
        "id": "1noMwhrcb9hh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXDay7l6GI_3",
        "outputId": "1dd30ea0-d0d5-44ea-f662-eb1cc672be38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.4\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bzi8KFytcq0W"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "path = 'drive/MyDrive/StructuralBioinformaticsUNIPD/Final Project'\n",
        "\n",
        "df = pd.read_csv(path + \"/clean_additional_features_dataset.csv\")\n",
        "df = df.drop(\"Unnamed: 0\", axis=1)\n",
        "\n",
        "# Remove all rows with NaN in at least one column\n",
        "# including rows with missing class (they could be false negatives)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# drop hydrophobicity feature since highly correlated with s_aa1\n",
        "df = df.drop(['s_residue_hydrophobicity', 't_residue_hydrophobicity'], axis=1)\n",
        "\n",
        "\n",
        "# Fit and transform the categorical columns\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_values = label_encoder.fit_transform(df['s_ss8'])\n",
        "df['s_ss8'] = encoded_values\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_values = label_encoder.fit_transform(df['s_ss3'])\n",
        "df['s_ss3'] = encoded_values\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_values = label_encoder.fit_transform(df['s_ss8'])\n",
        "df['t_ss8'] = encoded_values\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_values = label_encoder.fit_transform(df['t_ss3'])\n",
        "df['t_ss3'] = encoded_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3MdCan_HtXS4"
      },
      "outputs": [],
      "source": [
        "# Convert dataset to multilabeled by grouping entries by residue identifying columns, using one-hot encoding\n",
        "# in X_list and y_list we have the lists containing the dataset entries and labels, that will be used for training\n",
        "\n",
        "grouped_df = df.drop([\"pdb_id\"], axis=1).groupby(['s_ch', 's_resi', 's_ins', 's_resn', 't_ch', 't_resi', 't_ins', 't_resn' ])\n",
        "\n",
        "# List to store the values from all groups\n",
        "all_group_values = []\n",
        "X_list = []\n",
        "y_list = []\n",
        "\n",
        "label_dict = {\"HBOND\": 0,\n",
        "              \"IONIC\": 1,\n",
        "              \"PICATION\": 2,\n",
        "              \"PIPISTACK\": 3,\n",
        "              \"SSBOND\": 4,\n",
        "              \"VDW\": 5 }\n",
        "\n",
        "# Iterate over the groups and extract values\n",
        "for group_name, group_df in grouped_df:\n",
        "    values_within_group = group_df['Interaction'].tolist()\n",
        "    row = group_df.iloc[:1,:-1].drop(['s_ch', 's_resi', 's_ins', 's_resn', 't_ch', 't_resi', 't_ins', 't_resn'], axis =1).values\n",
        "    X_list.append(list(row[0]))\n",
        "\n",
        "    # one-hot encoding\n",
        "    labels_list = [0,0,0,0,0,0]\n",
        "    for label in values_within_group:\n",
        "      labels_list[label_dict[label]]= 1\n",
        "\n",
        "    # print(labels_list)\n",
        "    y_list.append(labels_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9zFVGi5dbit_"
      },
      "outputs": [],
      "source": [
        "# MLSMOTE - code for oversampling of the dataset\n",
        "# -*- coding: utf-8 -*-\n",
        "# Importing required Library\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "\n",
        "def get_tail_label(df):\n",
        "    \"\"\"\n",
        "    Give tail label colums of the given target dataframe\n",
        "\n",
        "    args\n",
        "    df: pandas.DataFrame, target label df whose tail label has to identified\n",
        "\n",
        "    return\n",
        "    tail_label: list, a list containing column name of all the tail label\n",
        "    \"\"\"\n",
        "    columns = df.columns\n",
        "    n = len(columns)\n",
        "    irpl = np.zeros(n)\n",
        "    for column in range(n):\n",
        "        irpl[column] = df[columns[column]].value_counts()[1]\n",
        "    irpl = max(irpl)/irpl\n",
        "    mir = np.average(irpl)\n",
        "    tail_label = []\n",
        "    for i in range(n):\n",
        "        if irpl[i] > mir:\n",
        "            tail_label.append(columns[i])\n",
        "    return tail_label\n",
        "\n",
        "def get_index(df):\n",
        "  \"\"\"\n",
        "  give the index of all tail_label rows\n",
        "  args\n",
        "  df: pandas.DataFrame, target label df from which index for tail label has to identified\n",
        "\n",
        "  return\n",
        "  index: list, a list containing index number of all the tail label\n",
        "  \"\"\"\n",
        "  tail_labels = get_tail_label(df)\n",
        "  index = set()\n",
        "  for tail_label in tail_labels:\n",
        "    sub_index = set(df[df[tail_label]==1].index)\n",
        "    index = index.union(sub_index)\n",
        "  return list(index)\n",
        "\n",
        "def get_minority_instace(X, y):\n",
        "    \"\"\"\n",
        "    Give minority dataframe containing all the tail labels\n",
        "\n",
        "    args\n",
        "    X: pandas.DataFrame, the feature vector dataframe\n",
        "    y: pandas.DataFrame, the target vector dataframe\n",
        "\n",
        "    return\n",
        "    X_sub: pandas.DataFrame, the feature vector minority dataframe\n",
        "    y_sub: pandas.DataFrame, the target vector minority dataframe\n",
        "    \"\"\"\n",
        "    index = get_index(y)\n",
        "    X_sub = X[X.index.isin(index)].reset_index(drop = True)\n",
        "    y_sub = y[y.index.isin(index)].reset_index(drop = True)\n",
        "    return X_sub, y_sub\n",
        "\n",
        "def nearest_neighbour(X):\n",
        "    \"\"\"\n",
        "    Give index of 5 nearest neighbor of all the instance\n",
        "\n",
        "    args\n",
        "    X: np.array, array whose nearest neighbor has to find\n",
        "\n",
        "    return\n",
        "    indices: list of list, index of 5 NN of each element in X\n",
        "    \"\"\"\n",
        "    nbs=NearestNeighbors(n_neighbors=5,metric='euclidean',algorithm='kd_tree').fit(X)\n",
        "    euclidean,indices= nbs.kneighbors(X)\n",
        "    return indices\n",
        "\n",
        "def MLSMOTE(X,y, n_sample):\n",
        "    \"\"\"\n",
        "    Give the augmented data using MLSMOTE algorithm\n",
        "\n",
        "    args\n",
        "    X: pandas.DataFrame, input vector DataFrame\n",
        "    y: pandas.DataFrame, feature vector dataframe\n",
        "    n_sample: int, number of newly generated sample\n",
        "\n",
        "    return\n",
        "    new_X: pandas.DataFrame, augmented feature vector data\n",
        "    target: pandas.DataFrame, augmented target vector data\n",
        "    \"\"\"\n",
        "    indices2 = nearest_neighbour(X)\n",
        "    n = len(indices2)\n",
        "    new_X = np.zeros((n_sample, X.shape[1]))\n",
        "    target = np.zeros((n_sample, y.shape[1]))\n",
        "    for i in range(n_sample):\n",
        "        reference = random.randint(0,n-1)\n",
        "        neighbour = random.choice(indices2[reference,1:])\n",
        "        all_point = indices2[reference]\n",
        "        nn_df = y[y.index.isin(all_point)]\n",
        "        ser = nn_df.sum(axis = 0, skipna = True)\n",
        "        target[i] = np.array([1 if val>2 else 0 for val in ser])\n",
        "        ratio = random.random()\n",
        "        gap = X.loc[reference,:] - X.loc[neighbour,:]\n",
        "        new_X[i] = np.array(X.loc[reference,:] + ratio * gap)\n",
        "    new_X = pd.DataFrame(new_X, columns=X.columns)\n",
        "    target = pd.DataFrame(target, columns=y.columns)\n",
        "    new_X = pd.concat([X, new_X], axis=0)\n",
        "    target = pd.concat([y, target], axis=0)\n",
        "    return new_X, target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aVRqtUJcT1jV"
      },
      "outputs": [],
      "source": [
        "# mlp for multi-label classification\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import(\n",
        "    classification_report,\n",
        "    matthews_corrcoef,\n",
        "    balanced_accuracy_score,\n",
        "    average_precision_score,\n",
        "    roc_auc_score,\n",
        ")\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "import keras\n",
        "\n",
        "\n",
        "def NN_model(n_inputs, n_outputs):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(64, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
        "\tmodel.add(Dense(32, activation='relu'))\n",
        "\tmodel.add(Dense(20,  activation='relu'))\n",
        "\tmodel.add(Dropout(0.1))\n",
        "\tmodel.add(Dense(n_outputs, activation='sigmoid'))\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam') #, metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mJWA5vHQf8ZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75d086b2-da15-4e06-8ac7-e647433b94f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "11219/11219 [==============================] - 29s 2ms/step - loss: 0.2155\n",
            "Epoch 2/100\n",
            "11219/11219 [==============================] - 26s 2ms/step - loss: 0.1890\n",
            "Epoch 3/100\n",
            "11219/11219 [==============================] - 26s 2ms/step - loss: 0.1834\n",
            "Epoch 4/100\n",
            "11219/11219 [==============================] - 27s 2ms/step - loss: 0.1804\n",
            "Epoch 5/100\n",
            "11219/11219 [==============================] - 23s 2ms/step - loss: 0.1787\n",
            "Epoch 6/100\n",
            "11219/11219 [==============================] - 25s 2ms/step - loss: 0.1773\n",
            "Epoch 7/100\n",
            "11219/11219 [==============================] - 24s 2ms/step - loss: 0.1761\n",
            "Epoch 8/100\n",
            "11219/11219 [==============================] - 23s 2ms/step - loss: 0.1753\n",
            "Epoch 9/100\n",
            "11219/11219 [==============================] - 26s 2ms/step - loss: 0.1746\n",
            "Epoch 10/100\n",
            "11219/11219 [==============================] - 23s 2ms/step - loss: 0.1740\n",
            "Epoch 11/100\n",
            "11219/11219 [==============================] - 24s 2ms/step - loss: 0.1735\n",
            "Epoch 12/100\n",
            "11219/11219 [==============================] - 23s 2ms/step - loss: 0.1730\n",
            "Epoch 13/100\n",
            "11219/11219 [==============================] - 24s 2ms/step - loss: 0.1725\n",
            "Epoch 14/100\n",
            "11219/11219 [==============================] - 23s 2ms/step - loss: 0.1723\n",
            "Epoch 15/100\n",
            "11219/11219 [==============================] - 24s 2ms/step - loss: 0.1719\n",
            "Epoch 16/100\n",
            "11219/11219 [==============================] - 25s 2ms/step - loss: 0.1715\n",
            "Epoch 17/100\n",
            "11219/11219 [==============================] - 25s 2ms/step - loss: 0.1712\n",
            "Epoch 18/100\n",
            "11219/11219 [==============================] - 25s 2ms/step - loss: 0.1710\n",
            "Epoch 19/100\n",
            "11219/11219 [==============================] - 24s 2ms/step - loss: 0.1707\n",
            "Epoch 20/100\n",
            "11219/11219 [==============================] - 23s 2ms/step - loss: 0.1708\n",
            "Epoch 21/100\n",
            "11219/11219 [==============================] - 24s 2ms/step - loss: 0.1702\n",
            "Epoch 22/100\n",
            "11219/11219 [==============================] - 23s 2ms/step - loss: 0.1701\n",
            "Epoch 23/100\n",
            "11219/11219 [==============================] - 24s 2ms/step - loss: 0.1701\n",
            "Epoch 24/100\n",
            "11219/11219 [==============================] - 23s 2ms/step - loss: 0.1698\n",
            "Epoch 25/100\n",
            "11219/11219 [==============================] - 26s 2ms/step - loss: 0.1697\n",
            "Epoch 26/100\n",
            "11219/11219 [==============================] - 24s 2ms/step - loss: 0.1695\n",
            "Epoch 27/100\n",
            "11219/11219 [==============================] - 23s 2ms/step - loss: 0.1693\n",
            "Epoch 28/100\n",
            "11219/11219 [==============================] - 24s 2ms/step - loss: 0.1691\n",
            "Epoch 29/100\n",
            "11219/11219 [==============================] - 22s 2ms/step - loss: 0.1689\n",
            "Epoch 30/100\n",
            "11219/11219 [==============================] - 24s 2ms/step - loss: 0.1689\n",
            "Epoch 31/100\n",
            "11219/11219 [==============================] - 24s 2ms/step - loss: 0.1687\n",
            "Epoch 32/100\n",
            "11219/11219 [==============================] - 25s 2ms/step - loss: 0.1687\n",
            "Epoch 33/100\n",
            "11219/11219 [==============================] - 26s 2ms/step - loss: 0.1685\n",
            "Epoch 34/100\n",
            "11219/11219 [==============================] - 23s 2ms/step - loss: 0.1684\n",
            "Epoch 35/100\n",
            "11219/11219 [==============================] - 24s 2ms/step - loss: 0.1685\n",
            "Epoch 36/100\n",
            "11219/11219 [==============================] - 24s 2ms/step - loss: 0.1683\n",
            "Epoch 37/100\n",
            "11219/11219 [==============================] - 23s 2ms/step - loss: 0.1681\n",
            "Epoch 38/100\n",
            "11219/11219 [==============================] - 24s 2ms/step - loss: 0.1679\n",
            "Epoch 39/100\n",
            "11219/11219 [==============================] - 23s 2ms/step - loss: 0.1680\n",
            "Epoch 40/100\n",
            "11219/11219 [==============================] - 26s 2ms/step - loss: 0.1677\n",
            "Epoch 41/100\n",
            "11219/11219 [==============================] - 24s 2ms/step - loss: 0.1679\n",
            "Epoch 42/100\n",
            "11219/11219 [==============================] - 23s 2ms/step - loss: 0.1677\n",
            "Epoch 43/100\n",
            "11219/11219 [==============================] - 25s 2ms/step - loss: 0.1674\n",
            "Epoch 44/100\n",
            "11219/11219 [==============================] - 23s 2ms/step - loss: 0.1677\n",
            "Epoch 45/100\n",
            "11219/11219 [==============================] - 25s 2ms/step - loss: 0.1674\n",
            "Epoch 46/100\n",
            "11219/11219 [==============================] - 24s 2ms/step - loss: 0.1675\n"
          ]
        }
      ],
      "source": [
        "# train the NN model\n",
        "\n",
        "import torch\n",
        "from torchmetrics.functional.classification import multilabel_matthews_corrcoef\n",
        "from tensorflow.keras.models import save_model\n",
        "\n",
        "def train_NN_model(X, y):\n",
        "\tn_inputs, n_outputs = X.shape[1], y.shape[1]\n",
        "\n",
        "\tX_train = X\n",
        "\ty_train = y\n",
        "\n",
        "\tX_sub, y_sub = get_minority_instace(pd.DataFrame(X_train), pd.DataFrame(y_train, columns=['HBOND', 'IONIC', 'PICATION', 'PIPISTACK', 'SSBOND', 'VDW']).astype(int))   #Getting minority instance of that datframe\n",
        "\tX_res,y_res =MLSMOTE(X_sub, y_sub, int(0.1*len(X_train)))\n",
        "\n",
        "\tX_train = np.concatenate((X_train, X_res[len(X_sub):].values))\n",
        "\ty_train = np.concatenate((y_train, y_res[len(y_sub):].values))\n",
        "\n",
        "\t# define model\n",
        "\tmodel = NN_model(n_inputs, n_outputs)\n",
        "\t# fit model\n",
        "\tearly_stopping = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
        "\thistory = model.fit(X_train, y_train, verbose=1, epochs=100, callbacks=[early_stopping])\n",
        "\n",
        "\tsave_model(model, path + '/final_model.h5')\n",
        "\n",
        "\n",
        "\treturn model\n",
        "\n",
        "# load dataset\n",
        "X1, y1 = np.array(X_list), np.array(y_list)\n",
        "# evaluate model\n",
        "model= train_NN_model(X1, y1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# just a check if model is saved and loaded properly\n",
        "X1, y1 = np.array(X_list), np.array(y_list)\n",
        "\n",
        "# Load the saved model\n",
        "model = tf.keras.models.load_model(path + '/final_model.h5')\n",
        "\n",
        "# Assuming you want to make a prediction on the first element of X1\n",
        "single_input = X1[0]  # Select the desired element\n",
        "\n",
        "# Reshape the input to match the expected shape by the model\n",
        "single_input = np.expand_dims(single_input, axis=0)\n",
        "\n",
        "print(single_input)\n",
        "\n",
        "# Make the prediction on the single input\n",
        "y_pred = model.predict(single_input)\n",
        "\n",
        "# Round the probabilities to class labels\n",
        "y_pred = np.round(y_pred)\n",
        "\n",
        "# Print the predicted class label\n",
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2Y1Bjo5Dzzb",
        "outputId": "10e6bacc-4eeb-40ea-bdc0-753a51f0a9e4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 6.          0.528       2.         13.         -1.611      -0.394\n",
            "   0.         -0.032       0.326       2.213       0.908       1.313\n",
            "   6.          0.356      12.         15.         -1.113      -0.35\n",
            "   0.          1.831      -0.561       0.533      -0.277       1.648\n",
            "   5.07838     5.18498936  8.75005207  0.          0.        ]]\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "[[1. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    }
  ]
}